\chapter{Results}

In this chapter speed measurements of our application will be presented as well as a few pictures of results.
Because we have not accelerated the computation over traditional processors discussion of the reasons will follow.
We will mention possible changes in design to speed up the execution.
Then some consideration how shall the algorithm that is well suited for the \mbox{Cell/B.E.} look like.
Chapter will be finished with comparison of complexity of programming for the \mbox{Cell/B.E.} and conventional processors.

\section{Speed measurements}

\par
Actual speed measurement is performed within \mbox{\emph{GenerateData}} method of the \mbox{\emph{FiniteDifferenceFilter}}.
Counter is started right after initialization phase (i.e. before main algorithm loop) and stopped right after the loop.
Program memory usage was tuned to use only really necessary amount of memory within the measured interval.
Valgrind's massive tool was used for the memory usage tuning.
This was necessary because of the PS3 limited memory, see \ref{ps3MemoryUsage}

\par
Beside this server memory usage tuning some changes was made also within client part.
There has been a special filter developed.
The filter shrinks data set to given size and cast its voxels to float.
Shrinking is performed by linear interpolation.
The purpose of the float casting is avoiding allocation of additional memory needed for float data set if performed on the server side.

\par
It is strange that command \emph{top} shows far bigger memory usage than Valgrind's massif.
We have not cared much about it but the idea is that \emph{top} shows all memory requested from system while massif shows exact memory used by process.

\par
Results of the speed measurement is summarized in Table \ref{tab:runresults}.
Every measurement was run with the same curvature and speed-scaling factors.
But with different initial distance, maximum iterations and seed parameters.
These parameters were set according to data set size.

\par
Three different data sets were measured.
All of them was CT images of skull that were scanned for anthropological purposes.
Measurement of some volumetric parameters of such images is valuable source of data for anthropologist.
Method implemented in our application could be used in praxis for such purposes if the application is quick enough.
Therefore speed-up of current methods is necessary.

\begin{table}
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
\multicolumn{7}{|c|}{Measurement results}\\
\hline
data set		&size		&seed		&init.		&max. 		&arch.		&time\\
			&		&		&dist-		&itera-		&		&spend\\
			&		&		&ance		&tions		&		&(sec)\\
\hline
\hline
3slices 		&512x512x3	&[256,256,1]	&40		&800		&\mbox{Cell/B.E.}	&16.48\\
(skull 1)		&		&		&		&		&i686	.	&1.89\\
\hline
\hline
skull 1			&256x256x80	&[128,110,20]	&20		&500		&\mbox{Cell/B.E.}	&471.89\\
			&		&		&		&		&i686		&95.23\\
\hline
\hline
skull 1			&256x256x80	&[128,110,20]	&4		&500		&\mbox{Cell/B.E.}	&325.93\\
			&		&		&		&		&i686		&61.74\\
\hline
\hline
skull 2			&256x256x80	&[128,110,20]	&4		&500		&\mbox{Cell/B.E.}	&319.47\\
			&		&		&		&		&i686		&55.88\\
\hline
\hline
skull 2			&256x256x80	&[128,128,40]	&4		&500		&\mbox{Cell/B.E.}	&366.63\\
			&		&		&		&		&i686		&76.9\\
\hline
\end{tabular}
\par
\caption[Measurement results]
{
Results of speed measurement.
The first difference is big probably because of the small data set and insufficient time to take advantage of parallelism of six SPE.
The second measurement shows that PC implementation is about five times faster.
The rest of measurements, performed on different data set and parameter configurations prove the coefficient five.
}
\label{tab:runresults}
\end{table}

\section{Reasons of slowdown and possible improvements}

\par
Porting the code to run on SPEs and distribution the calculations among the available SPEs is not sufficient to get more speed from the \mbox{Cell/B.E.} over traditional processors.
The additional speed-up porting phases are necessary to be performed.
But our program has another speed pitfalls.

\par
The biggest problem is CellNeighbourhood that represent a small segment of an image (the $3^3$ voxel matrix).
It is transferred for every layer item.
In some parts the both output and status image neighbourhoods are transferred.
We wanted to perform the transfer in scatter-gather manner through the DMA lists but we have faced some problems.
The DMA transfers and specially using DMA lists are designed for bigger amount of aligned data.
When used for small amounts (smaller that 16bytes per list item) performance goes down because of unaligned data transfer.
When data smaller than 16 bytes (quad-word) are being transferred, every single item is automatically aligned to quad-word address within local store buffer (see figure \ref{fg:automaticAlignOfSmallData}).

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{data/automaticAlignOfSmallData}
    \caption[Automatic align of small data]
{
  Illustration of transfer of data that are smaller that quad-word.
  Hardware automatically increase address within local store buffer in such way that every transferred item is quad-word aligned.
}
    \label{fg:automaticAlignOfSmallData}
\end{figure}

This increase required size of buffer that is needed for the transfer.
This can be partially solved by usage of multiple DMA lists (one for each quadword align).
This is illustrated in the figure \ref{fg:multipleDMAList}.
For details see \cite{DMAListIssues}.

\begin{figure}
    \centering
    \includegraphics[width=0.95\textwidth]{data/multipleDMAList}
    \caption[Multiple DMA list workaround]
{
  Workaround for transfer in smaller than quad-word chunks.
  Uses more that one DMA list.
  One per quad-word align (e.g. for single bytes 16 DMA lists are needed).
}
    \label{fg:multipleDMAList}
\end{figure}

We have adopted this workaround and used it within the neighbourhood transfer.
Because of automatic local store quadword address aligning we had to use a translation table.
This table maps position of actual neighbourhood members into the local store buffer.
This is working solution but overhead for SPE is incredible and thus useless (see cellNeigbourhood.tcc for details).

\par
\label{neighbourhoodDependecy}
Another problem is order of layer nodes and thus neighbourhoods processing.
When there are two nodes within one layer that are next to each other and processed subsequently.
Changes made to the first processed neighbourhood would not appear to already preloaded next one.
So another merging need to be performed.

\par
This coerced another changes to neighbourhood transfer and made the actual transfer unbearably expensive operation.

\par
\label{workDependecy}
Another pitfall is the situation when sibling nodes are processed by two different SPEs.
Adding nodes to layers is based on information from neighbours of the currently processed node.
So it is possible that two different SPE inserts the same nodes more time into one layer because they process sibling nodes i.e. overlapping neighbourhoods.
This makes no changes to output but additional overhead due to processing multiplied nodes.
Solution to this problem would be synchronization among the SPEs.

\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{data/png/pcSlice}
    \includegraphics[width=0.45\textwidth]{data/png/cellSlice}
    \caption[Comparison of one slice segmented on different architectures]{Comparison of one slice segmentation on different architectures.
The left was computed on PC (i686), the right on \mbox{Cell/B.E.}
Although the segmentation was run with the same parameters there is small difference between those two images.
The \mbox{Cell/B.E.} level set has not reached as far as the PC one.
It is because of duplicate nodes within layers.
}
    \label{fg:sliceComparison}
\end{figure}

\par
Additional improvements of neighbourhood transfers should be done to speed up the execution.
This corresponds to data transfer optimization step of porting process (see paragraph \ref{fg:appPorting}).
In our case this means radical simplification of neighbourhood transfers.
This transfers should take only a few instructions to be effective.
Within SDK examples they are performed by simple macros which is probably the most effective way.
In our case simplification if the neighbourhood transfer would mean utilization of bigger image parts transport within the DMA transfers to avoid automatic local store alignment for small data transfers.
This would mean a bigger local store array to store the image part but processing of nodes that are associated with the part would be somehow gathered and thus the count of transfers would be less.
Processing of nodes should take into account their spatial information when inserted into a layer and thus to gather the node processing on the bigger neighbourhood.
But such computation scheme would coerce complete redesign of application.

\section{Code and design complexity}

\par
\mbox{Cell/B.E.} programming means mainly programming for SPEs because of their performance and count.
Because of indirect memory access and need of usage of some multi-buffering memory transfer scheme the design is quite more complex over common processor.
With another limitation which is the local store size is the design of \mbox{Cell/B.E.} application a challenge.

\par
\mbox{Cell/B.E.} is designed for intensive computation programs.
For a programmer this means utilize all SPEs and all their features at the maximum level.
It is only possible for certain class of algorithm.
Let's call them streaming parallel algorithm.
But what is it? What is the definition?
Work on our application has showed us what the streaming parallel algorithm is not.
So by negation of features that slow down our program we could get definition of the streaming parallel algorithm:
\begin{enumerate}

\item{Streaming nature}
\par
Data of that program are uniform and can be processed in small pieces (chunks) which are mutually independent and which can fit in local store memory.
This means that for a one chunk processing only this chunk is necessary and not any part of other chunks.
And when this chunk is once processed is stored and never retrieved for processing again.

\par
For our application this is not true.
At least not for all the input data.
Processing of a sparse field layer, linked chain of nodes, meets the streaming nature.
But processing of parts of underlying images (neighbourhoods) associated with these nodes does not.
The neighbourhoods are mutually dependent see \ref{neighbourhoodDependecy}.

\item{Paralel nature}
\par
Input data can be divided into parts which are again mutually independent and which can be sent to particular cores for processing.

\par
For our application is again not true.
Work that is divided among the cores is not independent because of dependency of particular neighbourhoods.
See \ref{workDependecy} what causes problems.

\end{enumerate}

\par
When algorithm does not meet the streaming parallel definition then it should not be implemented or it must be changed.
This means e.g. to use different data structures.
In our case, change of the data structure to somehow gather node processing to specific image part would be the change of algorithm we implemented leading to performance gain.
But then it would not be the original algorithm any more.

\par
In contrast there are algorithms that fits the streaming parallel algorithm definition.
Except the examples in SDK that fits the definition e.g. thresholding fits the streaming parallel algorithm definition.
It operates on an image that has uniform data - pixels.
It apply simple condition on each pixel which result depends only on the processed pixel.
Processing can be divided into chunks.
These three features meets the streaming condition.
Moreover the data can be divided into independent sets that can be processed by multiple cores (the parallel nature).
Implementation of this algorithm can therefore result huge performance gain on the \mbox{Cell/B.E.} than conventional processors.

\par
Our work has showed importance of initial consideration and design phase.
When there are more algorithms that are solving desirable problem programmer should think carefully which one will be the best for porting to \mbox{Cell/B.E.}
First stage of the initial consideration should be model of the application implementing chosen algorithm.
Thing what kind of data are processed.
If they are uniform.
If they are divisible into chunks.
If the computing can be divided into independent parts i.e. what entity defines amount of work to be done.
These are question that leads to answer if the chosen algorithm is or is not the streaming parallel.
If the answer is no, implementation of that algorithm is rather worthless and will result such a suboptimal program as the ours.

\par
Another thing is the code complexity.
Performing the optimisation porting cycle steps utilise language intrinsics for different kind of special instruction usage of macros for variety of purposes.
These techniques increase actual code complexity whereas may decrease code readability.

\par
There is still another question to be answered.
Is implementation of an algorithm worth at all?
Since only some special products is equipped with \mbox{Cell/B.E.} actual data has to be sent to such machine for computing and results has to be sent back.
So only complex algorithm where performance gain would be bigger than the time spent in transfer of actual data set are worth to port.
The algorithm we have tried to implement is complex enough to be offloaded to compute on remote machine while the mentioned thresholding would not.
As soon as common machines such as notebooks, desktops are equipped with the \mbox{Cell/B.E.} processor even class of such simple image processing algorithms that is implemented in everyday-use software such as the thresholding or variety of masking, edge detections could take advantage of the processor.

\chapter{Conclusion}

\par
At the beginning we studied available literature to find out what is actually the \mbox{Cell/B.E.} what benefits it brings for what price.
What special features it has and what they are good for.
Then we have been trying to install SDK to start actual development process.
During this phase we faced some obstacles such as bugs, incompatibilities among tools, libraries that the tools use and even operating system vs. SDK incompatibilities.
So we had to go through variety of forums and other sources to find the solution.
As a side effect we improved our Linux knowledge.
Eventually we managed to install SDK and was able to start developing.
Then we tested variety of libraries, tools and other feature that the SDK brings.

\par
We have chosen sparse field algorithm implementing level set based segmentation to port to \mbox{Cell/B.E.} platform.
This is quite complex algorithm to test the platform's potential.
We adopted ITK implementation of that algorithm.
So we had to study ITK tool kit and its internals.
We have also incorporated the whole program into MedV4D framework.
That means we have implemented some new modules that allows using ITK and can offload some part of processing to another machine (that run on the \mbox{Cell/B.E.}).

\par
Actual porting process started with profiling of existing application.
This step have found out hot spots of the code which can be in turn offloaded to SPE to take advantage of \mbox{Cell/B.E.} potential.
But results was quite unexpected so another redesign of application followed.
In this new design almost whole original ITK pipeline was offloaded into SPE.
Big code restructuring was necessary to allow us to perform actual computations on SPE.
We have repeatedly debugged same kind of errors due to corruptions of stack memory caused by DMA transfers.
This has proved importance of usage memory checking tools.
Finally we have been able to run the whole algorithm on SPE and to measure time need for computations.

\par
The result of measurement showed that simple move the computation to SPE slows down the computations because of insufficient utilization of \mbox{Cell/B.E.} features.
We have identified some bottlenecks of the application and discussed possible solutions.
Implementation of these solutions would require whole application redesign using another data structure.
This changes would actually mean change of used algorithm.

\par
We have also discussed differences between programming conventional processors and \mbox{Cell/B.E.}
As well as question of actual algorithm complexity and worthiness of porting them to \mbox{Cell/B.E.}

\par
The \mbox{Cell/B.E.} platform is very interesting for its variations of use scenarios and ability of program tuning and customization.
We think the pallet of tools and features of the \mbox{Cell/B.E.} can make it interesting alternative to conventional processors whose lifetime is getting shorter due to limitations in manufacturing process.
\mbox{Cell/B.E.}'s great potential has already been proven but it is still waiting for wider spectrum of programmers.

\par
If the process of starting developing on the \mbox{Cell/B.E.} became simpler we believe much more new programmers would start using and programming it.
Nowadays there are plenty of information about the \mbox{Cell/B.E.} but somehow unsorted or out of date.
The best information source are documents shipped along with the SDK.
But they are targeted to contain all the information regardless the level of experience of the reader.
So when a programmer wants to start developing applications on the \mbox{Cell/B.E.} he would go trough plenty of that information before he can start actual work.
It is a pity there is total lack of information for PS3 users within SDK docs.
This is quite problem when a big part of beginners has PS3 available.
There is simply lack of some "cookbook for beginners" with practical information and some howtos.
We believe this work can be such a cookbook with some of practical information that potentially may help to some other programmers who would like to start developing for the \mbox{Cell/B.E.}
